Tomato diseases classification by Magnus Jensen
In this project I worked with a Kaggle dataset that consisted of 10 classes. I worked with Tensorflow. Overall, I think the project went well but as usual I encountered some issues that prohibited me implementing extra features. I did document pretty extensively in the notebooks so the extra notes here will be rather brief.
As has been discussed in the notebooks, I think I could have accomplished more if I could use the gpu, but I have not been able to make that feature work. My plan was to use optuna for model tuning but I soon realized that was way beyond my PCs recourses.
I have made some mistakes that I will mention now:
1) there were some data leakage since I ran the image_dataset_from_directory() when I resumed to train my model the next day. Only at the end did I realize that I would end up with some test data in the training data set. Why I don’t see this as a total deal breaker was that my previous models were not trained during several days and they got similar validation accuracy (actually better)
2) on that topic. I had unforeseen difficulties with tf.dataset. The feature become a common occurring problem namely that I would re-shuffle each time. I might have been worth it to just use numpy since that would have helped with the plotting and metrics calculations.
3) I tested on the test set during model training which is a pretty big no-no. Why I ended up doing that was at the start I had big uses with my smaller dataset. The issue turned out to be how I downsampled the images using image_dataset_from_directory(). I still don’t understand why this made the models perform so poorly. Technically I should have used a hold-out set. But also I tested on the test set in this case because I wanted to gather more data on my model performance and save it so that it could be presented.
4) Also technically, it is recommended that once you are satisfied with your model you retrain it on your whole dataset (i.e. train, val, and test). Though this is less important when you have a big dataset from the beginning. 
5) I don’t think the data augmentation ended up really helping. Also I never got clarity if the optimizers Adam or rmsprop. 
6) Small thing is that I should change the accuracy_dict and model list to dataFrames to make it more consistent. At least accuracy_dict would be easier to work with if the data was ordered.
7) I had plans for including an autoencoder somewhere in the pipeline. That part of the project did not get much momentum since I struggled so much with tf.dataset structure at the start, and all the guides I found did not use tf.dataset (not even tensorflow’s own guides). So this is something I learned to consider more in the future.
8) I did manage to get a very basic autoencoder running at the end but it is giving me the opposite results of what I am looking for. The reconstruction error is higher for the class that the model has seen before (i.e. the model should do a better job of reconstructing images from the class it has trained on). So either the differences between the classes are to subtle, or my model architecture is not suitable or I made some more fundamental error (e.g. calculating the loss wrong). The plan was to use the autoencoder as a preprocessing step as anomaly detection. I had the idea of training one autoencoder for each class and then see if I could try inputting a foreign image (not a leaf or a new leaf decease) and see if any of the model anomaly detectors would raise an alert.   

  
