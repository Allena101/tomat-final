{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea was to use an autoencoder and its reconstruction loss as a filter for testing if an image was put into te pipeline that did not belong to any of the trained classes. I did not succeed..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have to use numpy since tf.dataset keeps giving me label errors. I thought i was doing something wrong but later i found a guide that did it that way and for that person it worked. I never resolved it and had to switch over to using numpy instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (996, 256, 256, 3)\n",
      "Shape of labels (996,)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directory containing your image data\n",
    "# data_dir = 'EARLY_BLIGHT'\n",
    "data_dir = r\"C:\\Users\\Magnus\\Desktop\\code\\timeSeries\\EARLY_BLIGHT\\early_blight\"\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "\n",
    "# Initialize an empty list to store the image data\n",
    "image_data = []\n",
    "\n",
    "# Loop through the image files and load them into NumPy arrays\n",
    "for image_file in image_files:\n",
    "    img = Image.open(image_file)\n",
    "    img = img.resize((256, 256))  # Resize images to your desired dimensions\n",
    "    img_array = np.array(img)\n",
    "    image_data.append(img_array)\n",
    "\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "image_data = np.array(image_data)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1] if needed\n",
    "images = image_data / 255.0\n",
    "\n",
    "labels = np.ones(len(image_files))\n",
    "\n",
    "# Check the shape of the loaded data\n",
    "print(\"Shape of images:\", images.shape)\n",
    "print('Shape of labels', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (9, 256, 256, 3)\n",
      "Shape of labels: 9\n",
      "Image File names: ['EB_1.JPG', 'EB_2.JPG', 'EB_3.JPG', 'LB_1.JPG', 'LB_2.JPG', 'LB_3.JPG', 'y_1.JPG', 'y_2.JPG', 'y_3.JPG']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = r\"C:\\Users\\Magnus\\Desktop\\code\\timeSeries\\EARLY_BLIGHT\\early_blight_test\"\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "\n",
    "# Initialize an empty list to store the image data and corresponding file names\n",
    "image_data = []\n",
    "file_names = []\n",
    "\n",
    "# Loop through the image files, load them into NumPy arrays, and store file names\n",
    "for image_file in image_files:\n",
    "    img = Image.open(image_file)\n",
    "    img = img.resize((256, 256))  # Resize images to your desired dimensions\n",
    "    img_array = np.array(img)\n",
    "    image_data.append(img_array)\n",
    "    file_name = os.path.basename(image_file)  # Extract the file name\n",
    "    file_names.append(file_name)\n",
    "\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "image_data = np.array(image_data)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1] if needed\n",
    "images_test = image_data / 255.0\n",
    "\n",
    "# Use file names as labels\n",
    "labels_test = file_names\n",
    "\n",
    "# Check the shape of the loaded data\n",
    "print(\"Shape of images:\", images_test.shape)\n",
    "print('Shape of labels:', len(labels_test))\n",
    "print('Image File names:', file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names\n",
    "test_labels = [1,1,1,2,2,2,3,3,3]\n",
    "test_labels_arr = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "996\n",
      "996\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(len(labels))\n",
    "print(len(images_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common autoencoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "32/32 [==============================] - 117s 4s/step - loss: 0.0447\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 121s 4s/step - loss: 0.0203\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 121s 4s/step - loss: 0.0103\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 121s 4s/step - loss: 0.0084\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 119s 4s/step - loss: 0.0077\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 120s 4s/step - loss: 0.0074\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 118s 4s/step - loss: 0.0071\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 119s 4s/step - loss: 0.0070\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 119s 4s/step - loss: 0.0068\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 118s 4s/step - loss: 0.0068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1f45914d2a0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AnomalyDetector(Model):\n",
    "  def __init__(self):\n",
    "    super(AnomalyDetector, self).__init__()\n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Input(shape=(256, 256, 3)),\n",
    "      layers.Conv2D(128, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(64, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(32, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', strides=2),\n",
    "      layers.Conv2D(8, (3, 3), activation='relu', padding='same', strides=2)])\n",
    "\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Conv2DTranspose(8, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(16, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2DTranspose(128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "      layers.Conv2D(3, kernel_size=(3, 3), activation='sigmoid', padding='same')])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = AnomalyDetector()\n",
    "\n",
    "\n",
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
    "\n",
    "autoencoder.fit(images, images,\n",
    "                epochs=10,\n",
    "                shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 279ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 256, 256), dtype=float64, numpy=\n",
       "array([[[0.23638134, 0.15132243, 0.28400104, ..., 0.01050796,\n",
       "         0.07478106, 0.08139582],\n",
       "        [0.12345941, 0.13153644, 0.1701395 , ..., 0.09368209,\n",
       "         0.02750916, 0.1352342 ],\n",
       "        [0.18685597, 0.28792092, 0.18222582, ..., 0.0500705 ,\n",
       "         0.01887463, 0.03643409],\n",
       "        ...,\n",
       "        [0.02683432, 0.03428159, 0.03310477, ..., 0.0480013 ,\n",
       "         0.03099233, 0.06442416],\n",
       "        [0.03318992, 0.07462111, 0.04538211, ..., 0.04855513,\n",
       "         0.07777776, 0.02412888],\n",
       "        [0.0332695 , 0.03242588, 0.05320393, ..., 0.13460113,\n",
       "         0.02691292, 0.11565174]],\n",
       "\n",
       "       [[0.14675852, 0.15878056, 0.16537416, ..., 0.04812819,\n",
       "         0.09569802, 0.11155248],\n",
       "        [0.13094712, 0.1534641 , 0.16770033, ..., 0.02392326,\n",
       "         0.01148375, 0.3372366 ],\n",
       "        [0.13906558, 0.16810625, 0.17914663, ..., 0.00437351,\n",
       "         0.07138054, 0.02262161],\n",
       "        ...,\n",
       "        [0.0250698 , 0.02287699, 0.03236139, ..., 0.11766338,\n",
       "         0.02973766, 0.03991945],\n",
       "        [0.0270576 , 0.03331541, 0.03814352, ..., 0.02552803,\n",
       "         0.05812394, 0.05755335],\n",
       "        [0.02780337, 0.02814597, 0.02390537, ..., 0.02636265,\n",
       "         0.13953976, 0.14811687]],\n",
       "\n",
       "       [[0.25499293, 0.25231857, 0.24651621, ..., 0.04176901,\n",
       "         0.20112011, 0.09693481],\n",
       "        [0.26340448, 0.25979206, 0.2488757 , ..., 0.04172828,\n",
       "         0.114477  , 0.03451615],\n",
       "        [0.19013262, 0.19218525, 0.18473449, ..., 0.04171218,\n",
       "         0.06031156, 0.07180442],\n",
       "        ...,\n",
       "        [0.02131012, 0.02705363, 0.03847859, ..., 0.26581283,\n",
       "         0.15032476, 0.06760454],\n",
       "        [0.0232216 , 0.04148726, 0.05204606, ..., 0.08059027,\n",
       "         0.11470543, 0.12392206],\n",
       "        [0.01990596, 0.03232173, 0.04667833, ..., 0.03372059,\n",
       "         0.09123145, 0.08692734]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.05129003, 0.07776181, 0.08482278, ..., 0.08799938,\n",
       "         0.06741   , 0.07159599],\n",
       "        [0.07950523, 0.12001728, 0.13641223, ..., 0.0527229 ,\n",
       "         0.02867832, 0.04637655],\n",
       "        [0.09018735, 0.14269694, 0.15531181, ..., 0.02800452,\n",
       "         0.02062996, 0.03864051],\n",
       "        ...,\n",
       "        [0.01830646, 0.01541536, 0.03285664, ..., 0.01423756,\n",
       "         0.0297709 , 0.02199565],\n",
       "        [0.01472474, 0.04018538, 0.06427484, ..., 0.01330979,\n",
       "         0.01182817, 0.00715107],\n",
       "        [0.02606354, 0.02471376, 0.05342389, ..., 0.00505284,\n",
       "         0.00406407, 0.004778  ]],\n",
       "\n",
       "       [[0.15651175, 0.08457887, 0.04795877, ..., 0.06217667,\n",
       "         0.056491  , 0.08702022],\n",
       "        [0.11431882, 0.02981493, 0.0297888 , ..., 0.0287839 ,\n",
       "         0.01319236, 0.02209565],\n",
       "        [0.08577539, 0.02794542, 0.03571482, ..., 0.01710723,\n",
       "         0.02870534, 0.01404306],\n",
       "        ...,\n",
       "        [0.01571414, 0.0108989 , 0.01699382, ..., 0.01529321,\n",
       "         0.0198469 , 0.06726611],\n",
       "        [0.03112097, 0.04504607, 0.05245133, ..., 0.01908505,\n",
       "         0.03283687, 0.08717664],\n",
       "        [0.05277291, 0.06741537, 0.07766989, ..., 0.05413389,\n",
       "         0.07740702, 0.11646187]],\n",
       "\n",
       "       [[0.03999419, 0.06962331, 0.0783839 , ..., 0.04279456,\n",
       "         0.02982449, 0.01862009],\n",
       "        [0.06036988, 0.10289584, 0.1199502 , ..., 0.03345529,\n",
       "         0.01436294, 0.00562022],\n",
       "        [0.06963625, 0.12363026, 0.1374816 , ..., 0.0214808 ,\n",
       "         0.01256871, 0.00929819],\n",
       "        ...,\n",
       "        [0.00552407, 0.01046935, 0.01952377, ..., 0.01735868,\n",
       "         0.01601777, 0.00509913],\n",
       "        [0.01671635, 0.03963029, 0.05388485, ..., 0.0162751 ,\n",
       "         0.01429117, 0.01569661],\n",
       "        [0.02454458, 0.04598398, 0.05975572, ..., 0.02945045,\n",
       "         0.00787899, 0.00380331]]])>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(images_test)\n",
    "test_loss = tf.keras.losses.mae(reconstructions, images_test)\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08198355, 0.06307536, 0.07137079, 0.05261784, 0.06301957,\n",
       "       0.06448122, 0.05561505, 0.07138394, 0.05130228])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty NumPy array to store the per-image MAE values\n",
    "mae_per_image = np.zeros(images_test.shape[0])\n",
    "\n",
    "# Loop through each image and calculate the MAE\n",
    "for i in range(images_test.shape[0]):\n",
    "    mae_per_image[i] = np.mean(np.abs(reconstructions[i] - images_test[i]))\n",
    "\n",
    "# Now, 'mae_per_image' contains one MAE value per image\n",
    "mae_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=float64, numpy=\n",
       "array([0.08198355, 0.06307536, 0.07137079, 0.05261784, 0.06301957,\n",
       "       0.06448122, 0.05561505, 0.07138394, 0.05130228])>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(images_test)\n",
    "test_loss_per_image = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(tf.abs(reconstructions - images_test), axis=1), axis=1), axis=1)\n",
    "test_loss_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am almost sure that the images are in the order that they were in the original folder. The first 3 images are of the same class that the model trained on reducing and then reconstructing whilst the other 6 images or 2 before unseen classes. As you can see, again the reconstruction error is highest among the images that the model has already seen so i am doing something wrong.\n",
    "\n",
    "Its pointless to calculate a threshold since i already know the pattern is in the wrong direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying a different type of autoencotder to see if that improves things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (995, 256, 256, 3)\n",
      "Shape of labels (995,)\n"
     ]
    }
   ],
   "source": [
    "# data_dir = 'EARLY_BLIGHT'\n",
    "data_dir = r\"C:\\Users\\Magnus\\Desktop\\code\\timeSeries\\EARLY_BLIGHT\\early_blight\"\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "\n",
    "# Initialize an empty list to store the image data\n",
    "image_data = []\n",
    "\n",
    "# Loop through the image files and load them into NumPy arrays\n",
    "for image_file in image_files:\n",
    "    img = Image.open(image_file)\n",
    "    img = img.resize((256, 256))  # Resize images to your desired dimensions\n",
    "    img_array = np.array(img)\n",
    "    image_data.append(img_array)\n",
    "\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "image_data = np.array(image_data)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1] if needed\n",
    "images = image_data / 255.0\n",
    "\n",
    "labels = np.ones(len(image_files))\n",
    "\n",
    "# Check the shape of the loaded data\n",
    "print(\"Shape of images:\", images.shape)\n",
    "print('Shape of labels', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of images: (9, 256, 256, 3)\n",
      "Shape of labels: 9\n",
      "Image File names: ['EB_1.JPG', 'EB_2.JPG', 'EB_3.JPG', 'LB_1.JPG', 'LB_2.JPG', 'LB_3.JPG', 'y_1.JPG', 'y_2.JPG', 'y_3.JPG']\n"
     ]
    }
   ],
   "source": [
    "data_dir = r\"C:\\Users\\Magnus\\Desktop\\code\\timeSeries\\early_blight_test\\EB\"\n",
    "\n",
    "# List all image files in the directory\n",
    "image_files = [os.path.join(data_dir, filename) for filename in os.listdir(data_dir)]\n",
    "\n",
    "# Initialize an empty list to store the image data and corresponding file names\n",
    "image_data = []\n",
    "file_names = []\n",
    "\n",
    "# Loop through the image files, load them into NumPy arrays, and store file names\n",
    "for image_file in image_files:\n",
    "    img = Image.open(image_file)\n",
    "    img = img.resize((256, 256))  # Resize images to your desired dimensions\n",
    "    img_array = np.array(img)\n",
    "    image_data.append(img_array)\n",
    "    file_name = os.path.basename(image_file)  # Extract the file name\n",
    "    file_names.append(file_name)\n",
    "\n",
    "# Convert the list of NumPy arrays to a single NumPy array\n",
    "image_data = np.array(image_data)\n",
    "\n",
    "# Normalize the pixel values to the range [0, 1] if needed\n",
    "images_test = image_data / 255.0\n",
    "\n",
    "# Use file names as labels\n",
    "labels_test = file_names\n",
    "\n",
    "# Check the shape of the loaded data\n",
    "print(\"Shape of images:\", images_test.shape)\n",
    "print('Shape of labels:', len(labels_test))\n",
    "print('Image File names:', file_names)\n",
    "\n",
    "\n",
    "file_names\n",
    "test_labels = [1,1,1,2,2,2,3,3,3]\n",
    "test_labels_arr = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "995\n",
      "(256, 256, 3)\n",
      "995\n",
      "9\n",
      "(256, 256, 3)\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(images))\n",
    "print(images[0].shape)\n",
    "print(len(labels))\n",
    "print(len(images_test))\n",
    "print(images_test[0].shape)\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I try a different architecture to see if it improves the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input shape\n",
    "input_shape = (256, 256, 3)\n",
    "\n",
    "# Build encoder\n",
    "encoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=2, activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, activation='relu'),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, activation='relu'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # tf.keras.layers.Dense(units=512, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu')\n",
    "])\n",
    "\n",
    "latent_shape = (256,)\n",
    "\n",
    "\n",
    "# Build decoder\n",
    "decoder = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=256, activation='relu', input_shape=latent_shape),\n",
    "    tf.keras.layers.Dense(units=4096, activation='relu'),\n",
    "    tf.keras.layers.Reshape(target_shape=(16, 16, 16)),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=2, activation='sigmoid', padding='same')\n",
    "])\n",
    "\n",
    "# Build autoencoder\n",
    "autoencoder = tf.keras.Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential_44 (Sequential)  (None, 256)               15768864  \n",
      "                                                                 \n",
      " sequential_45 (Sequential)  (None, 256, 256, 3)       1230147   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16999011 (64.85 MB)\n",
      "Trainable params: 16999011 (64.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "32/32 [==============================] - 29s 857ms/step - loss: 0.0360\n",
      "Epoch 2/15\n",
      "32/32 [==============================] - 28s 864ms/step - loss: 0.0322\n",
      "Epoch 3/15\n",
      "32/32 [==============================] - 26s 803ms/step - loss: 0.0285\n",
      "Epoch 4/15\n",
      "32/32 [==============================] - 22s 697ms/step - loss: 0.0254\n",
      "Epoch 5/15\n",
      "32/32 [==============================] - 25s 768ms/step - loss: 0.0223\n",
      "Epoch 6/15\n",
      "32/32 [==============================] - 27s 840ms/step - loss: 0.0201\n",
      "Epoch 7/15\n",
      "32/32 [==============================] - 25s 794ms/step - loss: 0.0183\n",
      "Epoch 8/15\n",
      "32/32 [==============================] - 25s 787ms/step - loss: 0.0180\n",
      "Epoch 9/15\n",
      "32/32 [==============================] - 25s 772ms/step - loss: 0.0165\n",
      "Epoch 10/15\n",
      "32/32 [==============================] - 26s 798ms/step - loss: 0.0157\n",
      "Epoch 11/15\n",
      "32/32 [==============================] - 25s 773ms/step - loss: 0.0152\n",
      "Epoch 12/15\n",
      "32/32 [==============================] - 25s 789ms/step - loss: 0.0146\n",
      "Epoch 13/15\n",
      "32/32 [==============================] - 24s 740ms/step - loss: 0.0144\n",
      "Epoch 14/15\n",
      "32/32 [==============================] - 23s 721ms/step - loss: 0.0139\n",
      "Epoch 15/15\n",
      "32/32 [==============================] - 24s 762ms/step - loss: 0.0134\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train model\n",
    "history = autoencoder.fit(\n",
    "    images,images,\n",
    "    epochs=15,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 484ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9,), dtype=float64, numpy=\n",
       "array([0.10340108, 0.08843391, 0.10330547, 0.07726714, 0.10288679,\n",
       "       0.09639677, 0.07369517, 0.09185791, 0.05853234])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructions = autoencoder.predict(images_test)\n",
    "test_loss_per_image = tf.reduce_mean(tf.reduce_mean(tf.reduce_mean(tf.abs(reconstructions - images_test), axis=1), axis=1), axis=1)\n",
    "test_loss_per_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am almost sure that the images are in the order that they were in the original folder. The first 3 images are of the same class that the model trained on reducing and then reconstructing whilst the other 6 images or 2 before unseen classes. As you can see, again the reconstruction error is highest among the images that the model has already seen so i am doing something wrong.\n",
    "\n",
    "Its pointless to calculate a threshold since i already know the pattern is in the wrong direction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvTS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
